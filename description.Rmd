---
title: "A simple hierarchical Gaussian process model for housing prices in Finland"
author: "Ville Mäkinen"
date: "13.3.2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction 

This document describes a simple hierarchical Gaussian process model for housing prices in Finland. The objective of the model is provide a working example of a Gaussian process in the housing prices context in contrast to the previous [work](https://helda.helsinki.fi/handle/10138/314591). Moreover, a new, more detailed, data set was collected such that the group-level model is defined over all post code areas in Finland. The presented Gaussian process model now incorporates the geographical effects on the basis of the post code areas.

Any questions or comments can be sent to ville piste ka piste makinen ät gmail piste com. 

## Data

The data was collected via web scraping from the \emph{asuntojen.hintatieto.fi}-service of the Finnish Ministery of the Environment and the Housing Finance and Development Centre of Finland.  

```{r, echo = FALSE, out.width = '30%', fig.align = 'center', fig.cap = "Post code areas"}
knitr::include_graphics("/home/asdf/Desktop/asuntojen hinnat/git/simple_gp_model_for_housing_prices_w_post_codes/post codes map/map.png")
```

### Data usage

Data was split into three data sets; estimation set, 'in-sample' test set and 'out-of-sample' test set. The splits were done by separating 5 % of the post codes found in the raw data to form the out-of-sample test set. Next, out of the remaining data, 20 % of the available observations are set aside as the in-sample test set. The remaining observations are used as the estimation set. The motivation for separating a out-of-sample test set is to check whether the Gaussian process captures geographical properties of the data. 

### Variable preprocessing 

For the regression models, each explanatory variable is centered and scaled. Centering is done subtracting the estimation set means from the untreated variables. Scaling is done by dividing the centered variables by 2 standard deviations calculated from the estimation set.

The response variable $\log \text{Price}_i^*$ is calculated by taking the natural logarithm for the recorded sales prices and then subtracting the estimation set mean from the variable, i.e. 

\begin{equation}
\log \text{Price}_i^* = \log \text{Price}_i - \overline{\log \text{Price}}_{\text{Estimation set}}
\end{equation}

where the term $\overline{\log \text{Price}}_{\text{Estimation set}}$ is the mean log price calculated from the estimation set. The price predictions for the in-sample and out-of-sample test set prices are generated by first predicting the (centered) log prices for each observation, then adding the estimation set mean log price from the estimation set. 

## Models

Two models were estimated using the newly collected data. Since the response variable are centered, no (explicit) intercept terms are included.  

### Simple linear hierarchical model

#### Structure

The likelihood of the model is given as 

\begin{equation}
\log \text{Price}_i^* \sim \text{N}(\mu_i, \sigma^2)  \nonumber
\end{equation}

where the expected value $\mu_i$ is determined by the sum 

\begin{eqnarray}
\mu_i     & =   & \beta^{\text{PostCode}}_{j[i]} +  \nonumber \\
          &     & \beta_{\text{Sqm}} \text{SqmStandardized}_i  + \nonumber  \\
          &     & \beta_{\text{OwnPropertyDummy}} \text{OwnPropertyDummyStandardized}_i +  \nonumber \\
          &     & \beta_{\text{RowHouseDummy}} \text{RowHouseDummyStandardized}_i +  \nonumber \\
          &     & \beta_{\text{TownHouseDummy}} \text{TownHouseDummyStandardized}_i +  \nonumber \\
          &     & \beta_{\text{SaunaDummy}} \text{SaunaDummyStandardized}_i +  \nonumber \\
          &     & \beta_{\text{ConditionUnrecordedDummy}} \text{ConditionUnrecordedDummyStandardized}_i +  \nonumber \\
          &     & \beta_{\text{ConditionGoodDummy}} \text{ConditionGoodDummyStandardized}_i  +  \nonumber \\
          &     & \beta_{\text{ConditionAdequateDummy}} \text{ConditionAdequateDummyStandardized}_i +  \nonumber \\
          &     & \beta_{\text{AgeOfBuilding}}  \text{AgeOfBuildingStandardized}_i  \nonumber
\end{eqnarray}

where the term $\beta^{\text{PostCode}}_j$ is the group-specific coefficient defined for each post code. The group-level model parameter priors are the following:

\begin{eqnarray}
\beta^{\text{PostCode}}_j & \sim & \text{N}(0, \sigma^2_\text{PostCode}),  \nonumber \\
\sigma_\text{PostCode} & \sim & \text{Half-Cauchy}(0, 1).  \nonumber 
\end{eqnarray}

The prior distribution choices for the coefficients are 

\begin{eqnarray}
\beta_{\text{Sqm}} \sim \text{N}(0,1),  \nonumber \\
\beta_{\text{OwnPropertyDummy}} \sim \text{N}(0,1),  \nonumber \\
\beta_{\text{RowHouseDummy}} \sim \text{N}(0,1),  \nonumber \\
\beta_{\text{TownHouseDummy}} \sim \text{N}(0,1), \nonumber \\
\beta_{\text{SaunaDummy}} \sim \text{N}(0,1), \nonumber \\
\beta_{\text{ConditionUnrecordedDummy}} \sim \text{N}(0,1), \nonumber \\
\beta_{\text{ConditionGoodDummy}} \sim \text{N}(0,1), \nonumber \\
\beta_{\text{ConditionAdequateDummy}} \sim \text{N}(0,1), \nonumber \\
\beta_{\text{AgeOfBuilding}} \sim \text{N}(0,1) \nonumber
\end{eqnarray}

and the prior distribution for the residual variance parameter is 

\begin{equation}
\sigma \sim \text{Half-Cauchy}(0, 1).
\end{equation}

#### Estimates

```{=latex}

\begin{table}[ht]
\centering
\begin{tabular}{rrrrrrrrrrr}
  \hline
 & mean & se\_mean & sd & 2.5\% & 25\% & 50\% & 75\% & 97.5\% & n\_eff & Rhat \\ 
  \hline
$\beta_{\text{Sqm}}$ & 0.53 & 0.00 & 0.01 & 0.52 & 0.52 & 0.53 & 0.53 & 0.54 & 14020.54 & 1.00 \\ 
$\beta_{\text{OwnPropertyDummy}}$ & 0.05 & 0.00 & 0.01 & 0.04 & 0.05 & 0.05 & 0.06 & 0.07 & 12912.76 & 1.00 \\ 
$\beta_{\text{RowHouseDummy}}$ & 0.17 & 0.00 & 0.01 & 0.16 & 0.17 & 0.17 & 0.17 & 0.18 & 10617.36 & 1.00 \\ 
$\beta_{\text{TownHouseDummy}}$ & 0.24 & 0.00 & 0.01 & 0.22 & 0.23 & 0.24 & 0.24 & 0.25 & 9171.00 & 1.00 \\ 
$\beta_{\text{SaunaDummy}}$ & 0.12 & 0.00 & 0.01 & 0.10 & 0.11 & 0.12 & 0.12 & 0.13 & 18711.42 & 1.00 \\ 
$\beta_{\text{ConditionUnrecordedDummy}}$ & 0.24 & 0.00 & 0.01 & 0.22 & 0.23 & 0.24 & 0.25 & 0.26 & 8229.70 & 1.00 \\ 
$\beta_{\text{ConditionGoodDummy}}$ & 0.51 & 0.00 & 0.02 & 0.48 & 0.50 & 0.51 & 0.52 & 0.55 & 7862.10 & 1.00 \\ 
$\beta_{\text{ConditionAdequateDummy}}$ & 0.25 & 0.00 & 0.01 & 0.22 & 0.24 & 0.24 & 0.25 & 0.27 & 8115.02 & 1.00 \\ 
$\beta_{\text{AgeOfBuilding}}$ & -0.33 & 0.00 & 0.01 & -0.34 & -0.33 & -0.33 & -0.32 & -0.31 & 16061.16 & 1.00 \\ 
  $\sigma$ & 0.30 & 0.00 & 0.00 & 0.30 & 0.30 & 0.30 & 0.30 & 0.31 & 24067.47 & 1.00 \\ 
   $\sigma_\text{PostCode}$ & 0.67 & 0.00 & 0.02 & 0.63 & 0.65 & 0.66 & 0.68 & 0.70 & 404.78 & 1.00 \\ 
   \hline
\end{tabular}
\caption{Simple linear model coefficient estimates} 
\end{table}

```

The below figure plots the posterior distributions of the post code effects for post codes with the 30 highest and lowest mean effects under the simple linear hierarchical model.  

```{r echo=F, fig.align = 'center', fig.cap = "Posteriors for post code intercepts, simple linear hierarchical model"}
load(file = "/home/asdf/Desktop/asuntojen hinnat/git/simple_gp_model_for_housing_prices_w_post_codes/simple linear regression normal dist/post_code_effects_simple_lin_reg.RData")

highest_post_code_effects <- post_code_effects_simple_lin_reg[, head(order(colMeans(post_code_effects_simple_lin_reg), decreasing = T), 30)]
lowest_post_code_effects <- post_code_effects_simple_lin_reg[, head(order(colMeans(post_code_effects_simple_lin_reg), decreasing = F), 30)]

par(mfrow=c(1,2))

boxplot(highest_post_code_effects[, ncol(highest_post_code_effects):1], 
        horizontal = T, 
        outline = F, 
        axes = F,
        ylim = range(highest_post_code_effects))

title(main = "highest post code effects", 
      cex.main = 0.6, 
      line = -0)

axis(side = 1, 
     round(seq(from = min(highest_post_code_effects), 
               to = max(highest_post_code_effects), 
               length.out = 6), digits = 2), 
     cex.axis = 0.75)

axis(side = 2, 
     at = ncol(highest_post_code_effects):1, 
     labels = colnames(highest_post_code_effects),
     las = 2, 
     cex.axis = 0.5)

abline(h = ncol(highest_post_code_effects):1, lty = 2, col = 'lightgrey', lwd = 0.25)

boxplot(lowest_post_code_effects, 
        horizontal = T, 
        outline = F,
        axes = F, 
        ylim = range(lowest_post_code_effects))

title(main = "lowest post code effects", 
      cex.main = 0.6, 
      line = -0)

axis(side = 1, 
     round(seq(from = min(lowest_post_code_effects), 
               to = max(lowest_post_code_effects), 
               length.out = 6), digits = 2), 
     cex.axis = 0.75)

axis(side = 2, 
     at = 1:ncol(lowest_post_code_effects), 
     labels = colnames(lowest_post_code_effects),
     las = 2, 
     cex.axis = 0.5)
abline(h = 1:ncol(lowest_post_code_effects), lty = 2, col = 'lightgrey', lwd = 0.25)

par(mfrow=c(1,1))
```

### Gaussian process model

#### Structure

The likelihood of the model is given as 

\begin{equation}
\log \text{Price}_i^* \sim \text{N}(\mu_i, \sigma^2) \nonumber
\end{equation}

where the expected value $\mu_i$ is determined by the sum 

\begin{eqnarray}
\mu_i     & =   & \beta^{\text{PostCode}}_{j[i]} + \nonumber \\
          &     & \beta_{\text{Sqm}} \text{SqmStandardized}_i + \nonumber \\
          &     & \beta_{\text{OwnPropertyDummy}} \text{OwnPropertyDummyStandardized}_i + \nonumber \\
          &     & \beta_{\text{RowHouseDummy}} \text{RowHouseDummyStandardized}_i +  \nonumber \\
          &     & \beta_{\text{TownHouseDummy}} \text{TownHouseDummyStandardized}_i + \nonumber \\
          &     & \beta_{\text{SaunaDummy}} \text{SaunaDummyStandardized}_i + \nonumber \\
          &     & \beta_{\text{ConditionUnrecordedDummy}} \text{ConditionUnrecordedDummyStandardized}_i + \nonumber \\
          &     & \beta_{\text{ConditionGoodDummy}} \text{ConditionGoodDummyStandardized}_i  + \nonumber \\
          &     & \beta_{\text{ConditionAdequateDummy}} \text{ConditionAdequateDummyStandardized}_i + \nonumber \\
          &     & \beta_{\text{AgeOfBuilding}}  \text{AgeOfBuildingStandardized}_i \nonumber
\end{eqnarray}

where the term $\beta^{\text{PostCode}}_j$ is the group-specific coefficient defined for each post code. The group-specific coefficient are determined through a Gaussian process model such that 

\begin{equation}
\beta^{\text{PostCode}}_j \sim \text{Multivariate-Normal}(0, \Sigma) \nonumber
\end{equation}

where the covariance matrix $\Sigma$ is defined with the kernel 

\begin{equation}
K_{ij} = \alpha^2 \exp\left(-\frac{1}{2\rho^2} (\text{Distance between post code centroids i and j})^2\right) + \sigma^2_{\text{GP}} \delta_{ij} \nonumber
\end{equation}

where the terms $\alpha$, $\rho$ and $\sigma^2_{\text{GP}}$ are parameters and the term $\delta_{ij}$ is the Kronecker delta. The parameters have the following prior distributions: 

\begin{eqnarray}
\alpha & \sim & \text{Half-Cauchy}(0, 1), \nonumber  \\
\rho & \sim & \text{GeneralizedInverseGaussian(p = 1, a = 1, b = 1)} \nonumber
\end{eqnarray}

and

\begin{equation}
\sigma_{\text{GP}} \sim \text{Half-Cauchy}(0, 1). \nonumber
\end{equation}

Finally, the priors for the coefficients are 

\begin{eqnarray}
\beta_{\text{Sqm}} \sim \text{N}(0,1),  \nonumber \\
\beta_{\text{OwnPropertyDummy}} \sim \text{N}(0,1),  \nonumber \\
\beta_{\text{RowHouseDummy}} \sim \text{N}(0,1),  \nonumber \\
\beta_{\text{TownHouseDummy}} \sim \text{N}(0,1), \nonumber \\
\beta_{\text{SaunaDummy}} \sim \text{N}(0,1), \nonumber \\
\beta_{\text{ConditionUnrecordedDummy}} \sim \text{N}(0,1), \nonumber \\
\beta_{\text{ConditionGoodDummy}} \sim \text{N}(0,1), \nonumber \\
\beta_{\text{ConditionAdequateDummy}} \sim \text{N}(0,1), \nonumber \\
\beta_{\text{AgeOfBuilding}} \sim \text{N}(0,1) \nonumber
\end{eqnarray}

and the prior for the residual variance is 

\begin{equation}
\sigma \sim \text{Half-Cauchy}(0, 1). \nonumber
\end{equation}

#### Estimates

```{=latex}

\begin{table}[ht]
\centering
\begin{tabular}{rrrrrrrrrrr}
  \hline
 & mean & se\_mean & sd & 2.5\% & 25\% & 50\% & 75\% & 97.5\% & n\_eff & Rhat \\ 
  \hline
  $\beta_{\text{Sqm}}$ & 0.53 & 0.00 & 0.01 & 0.51 & 0.52 & 0.53 & 0.53 & 0.54 & 4046.05 & 1.00 \\ 
  $\beta_{\text{OwnPropertyDummy}}$ & 0.06 & 0.00 & 0.01 & 0.05 & 0.05 & 0.06 & 0.06 & 0.07 & 5104.99 & 1.00 \\ 
  $\beta_{\text{RowHouseDummy}}$ & 0.17 & 0.00 & 0.01 & 0.16 & 0.16 & 0.17 & 0.17 & 0.18 & 3800.83 & 1.00 \\ 
  $\beta_{\text{TownHouseDummy}}$  & 0.24 & 0.00 & 0.01 & 0.23 & 0.24 & 0.24 & 0.25 & 0.26 & 3143.38 & 1.00 \\ 
  $\beta_{\text{SaunaDummy}}$  & 0.12 & 0.00 & 0.01 & 0.11 & 0.11 & 0.12 & 0.12 & 0.13 & 5953.97 & 1.00 \\ 
  $\beta_{\text{ConditionUnrecordedDummy}}$ & 0.25 & 0.00 & 0.01 & 0.22 & 0.24 & 0.25 & 0.25 & 0.27 & 2248.21 & 1.00 \\ 
  $\beta_{\text{ConditionGoodDummy}}$ & 0.52 & 0.00 & 0.02 & 0.49 & 0.51 & 0.52 & 0.53 & 0.55 & 2228.87 & 1.00 \\ 
  $\beta_{\text{ConditionAdequateDummy}}$ & 0.25 & 0.00 & 0.01 & 0.22 & 0.24 & 0.25 & 0.26 & 0.28 & 2301.32 & 1.00 \\ 
  $\beta_{\text{AgeOfBuilding}}$ & -0.32 & 0.00 & 0.01 & -0.33 & -0.33 & -0.32 & -0.32 & -0.31 & 5426.88 & 1.00 \\ 
  $\sigma$ & 0.30 & 0.00 & 0.00 & 0.30 & 0.30 & 0.30 & 0.30 & 0.31 & 7456.27 & 1.00 \\ 
   \hline
\end{tabular}
\caption{Coefficient estimates} 
\end{table}


```



```{=latex}
\begin{table}[ht]
\centering
\begin{tabular}{rrrrrrrrrrr}
  \hline
 & mean & se\_mean & sd & 2.5\% & 25\% & 50\% & 75\% & 97.5\% & n\_eff & Rhat \\ 
  \hline
$\rho$ & 23.79 & 0.09 & 1.30 & 21.24 & 22.89 & 23.80 & 24.67 & 26.34 & 224.05 & 1.01 \\ 
  $\alpha$ & 0.77 & 0.00 & 0.05 & 0.67 & 0.73 & 0.76 & 0.80 & 0.88 & 239.72 & 1.01 \\ 
  $\sigma_{\text{GP}}$ & 0.20 & 0.00 & 0.01 & 0.19 & 0.20 & 0.20 & 0.21 & 0.22 & 508.41 & 1.00 \\ 
   \hline
\end{tabular}
\caption{GP model parameter estimates} 
\end{table}
```


The below figure plots the posterior distributions of the post code effects for post codes with the 30 highest and lowest mean effects under the Gaussian process model.  


```{r echo=F, fig.align = 'center', fig.cap = "Posteriors for post code intercepts, Gaussian process model"}
load(file = "/home/asdf/Desktop/asuntojen hinnat/git/simple_gp_model_for_housing_prices_w_post_codes/gp intercepts/post_code_effects_gp.RData")

highest_post_code_effects <- post_code_effects_gp[, head(order(colMeans(post_code_effects_gp), decreasing = T), 30)]
lowest_post_code_effects <- post_code_effects_gp[, head(order(colMeans(post_code_effects_gp), decreasing = F), 30)]

par(mfrow=c(1,2))

boxplot(highest_post_code_effects[, ncol(highest_post_code_effects):1], 
        horizontal = T, 
        outline = F, 
        axes = F,
        ylim = range(highest_post_code_effects))

title(main = "highest post code effects", 
      cex.main = 0.6, 
      line = -0)

axis(side = 1, 
     round(seq(from = min(highest_post_code_effects), 
               to = max(highest_post_code_effects), 
               length.out = 6), digits = 2), 
     cex.axis = 0.75)

axis(side = 2, 
     at = ncol(highest_post_code_effects):1, 
     labels = colnames(highest_post_code_effects),
     las = 2, 
     cex.axis = 0.5)

abline(h = ncol(highest_post_code_effects):1, lty = 2, col = 'lightgrey', lwd = 0.25)

boxplot(lowest_post_code_effects, 
        horizontal = T, 
        outline = F,
        axes = F, 
        ylim = range(lowest_post_code_effects))

title(main = "lowest post code effects", 
      cex.main = 0.6, 
      line = -0)

axis(side = 1, 
     round(seq(from = min(lowest_post_code_effects), 
               to = max(lowest_post_code_effects), 
               length.out = 6), digits = 2), 
     cex.axis = 0.75)

axis(side = 2, 
     at = 1:ncol(lowest_post_code_effects), 
     labels = colnames(lowest_post_code_effects),
     las = 2, 
     cex.axis = 0.5)
abline(h = 1:ncol(lowest_post_code_effects), lty = 2, col = 'lightgrey', lwd = 0.25)

par(mfrow=c(1,1))
```



## Results 

### LOO comparison 

```{r include=FALSE}
library(loo)

load(file = "/home/asdf/Desktop/asuntojen hinnat/git/simple_gp_model_for_housing_prices_w_post_codes/gp intercepts/loo_git_gp.RData")
load(file = "/home/asdf/Desktop/asuntojen hinnat/git/simple_gp_model_for_housing_prices_w_post_codes/simple linear regression normal dist/loo_fit_simple_lin_reg.RData")

print(loo_fit_simple_lin_reg)

loo_compare(loo_fit_gp, loo_fit_simple_lin_reg)

```

The loo-statistics printout for the Gaussian process model is the following:

```{r echo = F}
print(loo_fit_gp)
#pareto_k_table(loo_fit_gp)
```

The loo-statistics printout for the simple linear hierarchical model is the following: 
```{r echo = F}
print(loo_fit_simple_lin_reg)
#pareto_k_table(loo_fit_simple_lin_reg)
```

Both loo-statistics printouts indicate problems with the models: [Ideally](https://cran.r-project.org/web/packages/loo/vignettes/loo2-example.html), there shouldn't observations with pareto k greater than $0.7$ or these should be handled with exact LOO calculations. The some of these problematic observations are seemingly outliers with likely data entry issues, these include e.g. observations with price per square meters of 74 $e / m^2$ or buildings built in the year 1056. Perhaps the results could be (somewhat) improved by using a robust likelihood distribution. 

Model comparison figures indicate that the Gaussian process model outperforms the simple linear hierarchical model: 

```{r}
loo_compare(loo_fit_gp, loo_fit_simple_lin_reg)
```


### Predictive distributions

#### Calibration 

The PIT histograms indicate that neither model is well-calibrated - ideally the PIT histogram should be match the density of Uniform(0,1). 

```{r echo=FALSE, fig.align = 'center', fig.cap = "PIT histograms"}

load('/home/asdf/Desktop/asuntojen hinnat/git/simple_gp_model_for_housing_prices_w_post_codes/gp intercepts/PIT_data_gp.RData')
load('/home/asdf/Desktop/asuntojen hinnat/git/simple_gp_model_for_housing_prices_w_post_codes/simple linear regression normal dist/PIT_data_simple_lin_reg.RData')

par(mfrow = c(1,2))

hist(training_data_PIT_transform_gp, 
     probability = T,
     main = "PIT histogram,\nGaussian process model", 
     xlab = "probability integral transform")
abline(h = 1, lty = 2)

hist(training_data_PIT_transform_simple_lin_reg, 
     probability = T, 
     main = "PIT histogram,\nsimple lin. hier. model", 
     xlab = "probability integral transform")
abline(h = 1, lty = 2)

par(mfrow = c(1,1))


```

#### Point predictions 

```{r include=F}
load('/home/asdf/Desktop/asuntojen hinnat/git/simple_gp_model_for_housing_prices_w_post_codes/gp intercepts/test_set_pred_dist_list_gp.RData')
load('/home/asdf/Desktop/asuntojen hinnat/git/simple_gp_model_for_housing_prices_w_post_codes/simple linear regression normal dist/test_set_pred_dist_list_simple_lin_reg.RData')
```

Point predictions are generated by taking the means from the predictive distribution samples. 

##### Point predictions for in-sample test set 

```{r include=F}


in_sample_pred_dist_draws_gp <- test_set_pred_dist_list_gp$in_sample_pred_dist_draws
in_sample_pred_dist_draws_simple_lin_reg <- test_set_pred_dist_list_simple_lin_reg$in_sample_pred_dist_draws

out_of_sample_pred_dist_draws_gp <- test_set_pred_dist_list_gp$out_of_sample_pred_dist_draws
out_of_sample_pred_dist_draws_simple_lin_reg <- test_set_pred_dist_list_simple_lin_reg$out_of_sample_pred_dist_draws

in_sample_log_price_point_pred_gp <- colMeans(in_sample_pred_dist_draws_gp)
in_sample_log_price_point_pred_simple_lin_reg <- colMeans(in_sample_pred_dist_draws_simple_lin_reg)

out_of_sample_log_price_point_pred_gp <- colMeans(out_of_sample_pred_dist_draws_gp)
out_of_sample_log_price_point_pred_simple_lin_reg <- colMeans(out_of_sample_pred_dist_draws_simple_lin_reg)

in_sample_test_data <- read.csv2("/home/asdf/Desktop/asuntojen hinnat/git/simple_gp_model_for_housing_prices_w_post_codes/data/in_sample_test_data.csv", 
                                 stringsAsFactors = F)

out_of_sample_test_data <- read.csv2("/home/asdf/Desktop/asuntojen hinnat/git/simple_gp_model_for_housing_prices_w_post_codes/data/geographical_test_data.csv", 
                                     stringsAsFactors = F)

# post code "46860" moved to OOS - some problem with the data construction?
out_of_sample_test_data <- rbind(out_of_sample_test_data, 
                                 in_sample_test_data[in_sample_test_data$post_code == "46860",])
in_sample_test_data <- in_sample_test_data[in_sample_test_data$post_code != "46860",]

```

From the figure below it can be seen that the point predictions are essentially the same for both models for the test set with the in-sample post codes. 

```{r, echo=F, fig.align = 'center', fig.cap = "In-sample test set point predictions"}

par(mfrow = c(1, 2))
plot(in_sample_log_price_point_pred_gp, 
     log(in_sample_test_data$price),
     xlab = "log price point pred.", 
     ylab = "obs. log price", 
     main = "Gaussian process model,\nin-sample test set", 
     ylim = c(8, 15), 
     xlim = c(8, 15))
abline(a = 0, 
       b = 1, 
       col = 'red', 
       lty = 2, 
       lwd = 2)

plot(in_sample_log_price_point_pred_simple_lin_reg, 
     log(in_sample_test_data$price),
     xlab = "log price point pred.", 
     ylab = "obs. log price", 
     main = "simple lin. hier. mod.,\nin-sample test set", 
     ylim = c(8, 15), 
     xlim = c(8, 15))

abline(a = 0, 
       b = 1, 
       col = 'red', 
       lty = 2, 
       lwd = 2)

par(mfrow = c(1, 1))

```

##### Point predictions for out-of-sample test set 

For the out-of-sample post codes, the point predictions of the Gaussian process model match the observed prices somewhat better than the point predictions from the simple linear hierarchical model. Thus the Gaussian process approach captures at least some of the spatial effects of the true data generating process.  

```{r echo = F, fig.align = 'center', fig.cap = "Out-of-sample test set point predictions"}

par(mfrow = c(1, 2))
plot(out_of_sample_log_price_point_pred_gp, 
     log(out_of_sample_test_data$price),
     xlab = "log price point pred.", 
     ylab = "obs. log price", 
     main = "Gaussian process model,\nout-of-sample test set", 
     ylim = c(8, 15), 
     xlim = c(8, 15))
abline(a = 0, 
       b = 1, 
       col = 'red', 
       lty = 2, 
       lwd = 2)

plot(out_of_sample_log_price_point_pred_simple_lin_reg, 
     log(out_of_sample_test_data$price),
     xlab = "log price point pred.", 
     ylab = "obs. log price", 
     main = "simple lin. hier. mod.,\nout-of-sample test set", 
     ylim = c(8, 15), 
     xlim = c(8, 15))

abline(a = 0, 
       b = 1, 
       col = 'red', 
       lty = 2, 
       lwd = 2)

par(mfrow = c(1, 1))

```



The graphs below depict the point predictions for specific post codes. The graphs show that there are post codes where the Gaussian process approach is necessary (e.g. 00260, 02330, 88610) as well as post codes where the Gaussian process produces worse predictions (e.g. 20720).




```{r echo = F, fig.align = 'center', fig.cap = "Out-of-sample test set point predictions, contd."}

target_vec <- out_of_sample_test_data$post_code == 260 |
   out_of_sample_test_data$post_code == 370 |
   out_of_sample_test_data$post_code == 400 |
  out_of_sample_test_data$post_code == 88610 |
   out_of_sample_test_data$post_code == 87200
  
color_vec <- as.numeric(factor(out_of_sample_test_data$post_code))

remap_colors <- function(input_col_vec) {
  return(as.numeric(factor(input_col_vec)))
}

pad_postcode <- function(x) {
  if (nchar(x) == 5) {
    return(x)
  }
  
  padding_length <- 5 - nchar(x)
  
  padding <- paste(rep("0", padding_length), collapse="")
  return(paste(padding, x, collapse = "", sep = ""))
}

legend_info <- unique(data.frame(
  post_code = sapply(as.character(out_of_sample_test_data$post_code[target_vec]), pad_postcode),
  color = remap_colors(color_vec[target_vec])
))


par(mfrow = c(1, 2), mar = c(5,4,5,2) + 0.1)
plot(out_of_sample_log_price_point_pred_gp[target_vec], 
     log(out_of_sample_test_data$price[target_vec]),
     xlab = "log price point pred.", 
     ylab = "obs. log price",
     main = "Gaussian process model,\nout-of-sample test set",
     col = remap_colors(color_vec[target_vec]))
abline(a = 0,  
       b = 1, 
       col = 'red', 
       lty = 2, 
       lwd = 2)
legend("bottomright", 
       legend = legend_info$post_code,
       col = legend_info$color, 
       pch = 1, 
       cex = 0.5)

plot(out_of_sample_log_price_point_pred_simple_lin_reg[target_vec], 
     log(out_of_sample_test_data$price[target_vec]),
     xlab = "log price point pred.", 
     ylab = "obs. log price",
     main = "simple lin. hier. mod.,\nout-of-sample test set",
     col = remap_colors(color_vec[target_vec]))
abline(a = 0, 
       b = 1, 
       col = 'red', 
       lty = 2, 
       lwd = 2)
legend("bottomright", 
       legend = legend_info$post_code,
       col = legend_info$color, 
       pch = 1, 
       cex = 0.5)

par(mfrow = c(1, 1), mar = c(5,4,4,2) + 0.1)

```




```{r echo = F, fig.align = 'center', fig.cap = "Out-of-sample test set point predictions, contd."}

target_vec <- out_of_sample_test_data$post_code == 1400 |
  out_of_sample_test_data$post_code == 1900 |
  out_of_sample_test_data$post_code == 2330 |
  out_of_sample_test_data$post_code == 4200
  
color_vec <- as.numeric(factor(out_of_sample_test_data$post_code))

remap_colors <- function(input_col_vec) {
  return(as.numeric(factor(input_col_vec)))
}

pad_postcode <- function(x) {
  if (nchar(x) == 5) {
    return(x)
  }
  
  padding_length <- 5 - nchar(x)
  
  padding <- paste(rep("0", padding_length), collapse="")
  return(paste(padding, x, collapse = "", sep = ""))
}

legend_info <- unique(data.frame(
  post_code = sapply(as.character(out_of_sample_test_data$post_code[target_vec]), pad_postcode),
  color = remap_colors(color_vec[target_vec])
))


par(mfrow = c(1, 2), mar = c(5,4,5,2) + 0.1)
plot(out_of_sample_log_price_point_pred_gp[target_vec], 
     log(out_of_sample_test_data$price[target_vec]),
     xlab = "log price point pred.", 
     ylab = "obs. log price",
     main = "Gaussian process model,\nout-of-sample test set",
     col = remap_colors(color_vec[target_vec]))
abline(a = 0,  
       b = 1, 
       col = 'red', 
       lty = 2, 
       lwd = 2)
legend("bottomright", 
       legend = legend_info$post_code,
       col = legend_info$color, 
       pch = 1, 
       cex = 0.5)

plot(out_of_sample_log_price_point_pred_simple_lin_reg[target_vec], 
     log(out_of_sample_test_data$price[target_vec]),
     xlab = "log price point pred.", 
     ylab = "obs. log price",
     main = "simple lin. hier. mod.,\nout-of-sample test set",
     col = remap_colors(color_vec[target_vec]))
abline(a = 0, 
       b = 1, 
       col = 'red', 
       lty = 2, 
       lwd = 2)
legend("bottomright", 
       legend = legend_info$post_code,
       col = legend_info$color, 
       pch = 1, 
       cex = 0.5)


par(mfrow = c(1, 1), mar = c(5,4,4,2) + 0.1)

```




```{r echo = F, fig.align = 'center', fig.cap = "Out-of-sample test set point predictions, contd."}

target_vec <- out_of_sample_test_data$post_code == 20720 |
  out_of_sample_test_data$post_code == 40270 |
  out_of_sample_test_data$post_code == 49400 |
  out_of_sample_test_data$post_code == 4200
  
color_vec <- as.numeric(factor(out_of_sample_test_data$post_code))

remap_colors <- function(input_col_vec) {
  return(as.numeric(factor(input_col_vec)))
}

pad_postcode <- function(x) {
  if (nchar(x) == 5) {
    return(x)
  }
  
  padding_length <- 5 - nchar(x)
  
  padding <- paste(rep("0", padding_length), collapse="")
  return(paste(padding, x, collapse = "", sep = ""))
}

legend_info <- unique(data.frame(
  post_code = sapply(as.character(out_of_sample_test_data$post_code[target_vec]), pad_postcode),
  color = remap_colors(color_vec[target_vec])
))


par(mfrow = c(1, 2), mar = c(5,4,5,2) + 0.1)
plot(out_of_sample_log_price_point_pred_gp[target_vec], 
     log(out_of_sample_test_data$price[target_vec]),
     xlab = "log price point pred.", 
     ylab = "obs. log price",
     main = "Gaussian process model,\nout-of-sample test set",
     col = remap_colors(color_vec[target_vec]))
abline(a = 0,  
       b = 1, 
       col = 'red', 
       lty = 2, 
       lwd = 2)
legend("bottomright", 
       legend = legend_info$post_code,
       col = legend_info$color, 
       pch = 1, 
       cex = 0.5)


plot(out_of_sample_log_price_point_pred_simple_lin_reg[target_vec], 
     log(out_of_sample_test_data$price[target_vec]),
     xlab = "log price point pred.", 
     ylab = "obs. log price",
     main = "simple lin. hier. mod.,\nout-of-sample test set",
     col = remap_colors(color_vec[target_vec]))
abline(a = 0, 
       b = 1, 
       col = 'red', 
       lty = 2, 
       lwd = 2)
legend("bottomright", 
       legend = legend_info$post_code,
       col = legend_info$color, 
       pch = 1, 
       cex = 0.5)

par(mfrow = c(1, 1), mar = c(5,4,4,2) + 0.1)

```



## Shiny app for price predictions for the GP model 

https://ville-makinen.shinyapps.io/gp_predictions_shiny_app/
